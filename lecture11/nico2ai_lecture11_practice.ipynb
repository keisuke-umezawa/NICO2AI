{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICO2AI Reinforcement Learning I Practice\n",
    "\n",
    "## Grid World with Q Learning\n",
    "Grid World is a 2D environment described as 2D matrix. OpenAI Gym also provides a kind of grid world named FrozenLake with 4x4 matrix. But it is stochastic environment which is not effective to learn Q learning. So we have original grid world with the same interface as Gym. The objective of this world is getting to the goal with avoiding holes which give the negative reward. Let's see the reinforcement learning settings.\n",
    "\n",
    "- state: integer value representing position in the grid (0~15)\n",
    "- reward: integer value representing reward (0/1/-1)\n",
    "- action: integer value representing direction to move (0~3)\n",
    "- value: Q value represented by state-action table\n",
    "\n",
    "In addition, to make things more interesting, let's add small negative reward at each time step and change the value to see how it affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self):\n",
    "        self.grid = [\n",
    "            ['s', 's', 's', 's'],\n",
    "            ['s', 'h', 's', 'h'],\n",
    "            ['s', 's', 's', 'h'],\n",
    "            ['h', 's', 's', 'g']\n",
    "        ]\n",
    "        self.position = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.position = 0\n",
    "        return 0\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0 and (self.position + 1) % 4 != 0:\n",
    "            self.position += 1\n",
    "        elif action == 1 and self.position % 4 != 0:\n",
    "            self.position -= 1\n",
    "        elif action == 2 and self.position < 12:\n",
    "            self.position += 4\n",
    "        elif action == 3 and self.position > 4:\n",
    "            self.position -= 4\n",
    "        obj = self.grid[int(self.position / 4)][self.position % 4]\n",
    "        if obj == 's':\n",
    "            reward = 0\n",
    "        elif obj == 'h':\n",
    "            reward = -1\n",
    "        elif obj == 'g':\n",
    "            reward = 1\n",
    "        done = obj == 'g' or obj == 'h'\n",
    "        return self.position, reward, done, self.grid\n",
    "\n",
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have GridWorld environment just like previous bandit exercise. Fill blanks to complete Q learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, num_states, num_actions, discount=0.99, lr=0.1, epsilon=0.3):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.discount = discount\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.table = np.zeros((num_states, num_actions), dtype=np.float32)\n",
    "    \n",
    "    def act(self, state, greedy=False):\n",
    "        # epsilon-greedy action selection\n",
    "        # code here\n",
    "        # if greedy is True, this returns greedy action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        q = # q value\n",
    "        next_q = # next_q should be zero when the end of episode\n",
    "        td_error = # td error\n",
    "        self.table[state][action] += # update q table\n",
    "        return td_error\n",
    "    \n",
    "    def reset(self):\n",
    "        self.table = np.zeros_like(self.table, dtype=np.float32)\n",
    "    \n",
    "    def render_table(self):\n",
    "        plt.imshow(self.normalize_table() * 255.0)\n",
    "    \n",
    "    def render_path(self):\n",
    "        plt.imshow(np.max(self.normalize_table(), axis=1).reshape(4, 4))\n",
    "    \n",
    "    def normalize_table(self):\n",
    "        table = self.table - np.min(self.table)\n",
    "        table = table / np.max(table)\n",
    "        return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have learnable agent with Q learning. Then let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these values later\n",
    "discount = 0.9\n",
    "lr = 0.1\n",
    "epsilon = 0.3\n",
    "step_reward = -0.01\n",
    "\n",
    "agent = Agent(16, 4, discount, lr, epsilon)\n",
    "\n",
    "rewards = []\n",
    "for i in range(1000):\n",
    "    state = env.reset()\n",
    "    last_state = None\n",
    "    sum_of_reward = 0\n",
    "    while True:\n",
    "        # code here\n",
    "    rewards.append(sum_of_reward)\n",
    "\n",
    "# plot q table\n",
    "agent.render_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot max action values\n",
    "agent.render_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot moving average of rewards\n",
    "plt.plot(np.arange(1000), np.convolve(rewards, np.ones(10) / 10, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You get expected results? If yes, change `discount`, `lr`, `epsilon` and `step_reward` to see how they affect performance and state-action table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole with Q Learning\n",
    "CartPole is the well-known task that you control the cart to keep pole upright. First of all, run the following code to see CartPole animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "\n",
    "## util function to render CartPole as GIF image\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "frames = []\n",
    "while True:\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    _, _, done, _ = env.step(0)\n",
    "    if done:\n",
    "        break\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole is offered by OpenAI Gym. RL agent should feed +1 or -1 to the cart. A reward of +1 is provided for every timestep while the pole remains upright. The episode ends when the pole is more than 15 degrees from vertival, or the cart moves more than 2.4 units from the center. In this environemnt, reinforcement learning setting is described as below.\n",
    "\n",
    "- state: array with 4 elements representing (horizontal coordinate, horizontal velocity, pole angle, pole angular velocity)\n",
    "- action: integer value representing direction to push the cart (0/1)\n",
    "- reward: +1 for every timestep\n",
    "- value: Q value represented by state-action table\n",
    "\n",
    "The difference between GridWorld and CartPole is that state is continuous, not discrete (0.1, 0.001, 0.0001, ...). To deal with this problem, discretize state space by using `np.linspace(start, stop, num)` and `np.digitize(x, bins)`. `np.linspace(start, stop, num)` returns values between `start` and `stop` divided into `num`. `np.digitize(x, bins)` returns index which satisfies `bins[index - 1]` =< `x` < `bins[index]`. Let's make state transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change these values later\n",
    "horizontal_position_num = 2\n",
    "horizontal_velocity_num = 10\n",
    "pole_angle_num = 50\n",
    "pole_angular_velocity_num = 20\n",
    "\n",
    "horizontal_position_bins = np.linspace(-2.4, 2.4, horizontal_position_num)\n",
    "horizontal_velocity_bins = np.linspace(-2.0, 2.0, horizontal_velocity_num)\n",
    "pole_angle_bins = np.linspace(-0.4, 0.4, pole_angle_num)\n",
    "pole_angular_velocity_bins = np.linspace(-3.5, 3.5, pole_angular_velocity_num)\n",
    "\n",
    "def discretize(state):\n",
    "    # code here\n",
    "    # use np.digitize\n",
    "    # this should return discrete index out of 2*10*50*20 in default.\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can discretize continuous action space. In default settings, state space has the size of 2x10x50x20. Then fill blanks to train agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these values later\n",
    "discount = 0.9\n",
    "lr = 0.1\n",
    "epsilon = 0.1\n",
    "\n",
    "num_state = horizontal_position_num *\\\n",
    "        horizontal_velocity_num * pole_angle_num * pole_angular_velocity_num\n",
    "\n",
    "agent = Agent(num_state, 2, discount, lr, epsilon)\n",
    "\n",
    "rewards = []\n",
    "for i in range(10000):\n",
    "    state = env.reset()\n",
    "    last_state = None\n",
    "    sum_of_rewards = 0\n",
    "    \n",
    "    while True:\n",
    "        # code here\n",
    "    rewards.append(sum_of_rewards)\n",
    "\n",
    "# this may take a few minutes\n",
    "plt.plot(np.arange(10000), np.convolve(rewards, np.ones(100) / 100, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If agent is successfully trained, you would see the graph rising to the right. Finally, let's check agent behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate trained agent\n",
    "state = env.reset()\n",
    "frames = []\n",
    "t = 0\n",
    "while True:\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = agent.act(discretize(state), greedy=True)\n",
    "    state, _, done, _ = env.step(action)\n",
    "    t += 1\n",
    "    if done or t > 200:\n",
    "        break\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation!! You've mastered simple Q learning agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
