{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7回：畳み込みニューラルネットワーク (2)\n",
    "\n",
    "## 到達目標\n",
    "* 畳み込みニューラルネットワーク (CNN) が開発されるまでの歴史を概観する\n",
    "* CNNを用いて画像分類タスクを実装・実行できる\n",
    "\n",
    "## キーワード\n",
    "* 単純型・複雑型細胞\n",
    "* 畳み込み層\n",
    "* プーリング層\n",
    "* 全結合層\n",
    "* 特徴マップ\n",
    "* AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. 畳み込みニューラルネットの基礎\n",
    "##### 講義 (40分)\n",
    "* 畳込みニューラルネット (CNN)\n",
    "* 一般的なCNNの構成\n",
    "* 畳み込み層\n",
    "* パディング/ストライド\n",
    "* プーリング層 (Max pooling/average pooling)\n",
    "* チャネルと特徴マップ\n",
    "* 局所反応正規化 (Local Response Normalization; LRN)\n",
    "* 全結合層\n",
    "* 畳み込み層の誤差逆伝播法\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. 畳み込みニューラルネットの基礎\n",
    "##### 講義 (40分)\n",
    "* 畳込みニューラルネット (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 一般的なCNNの構成\n",
    "\n",
    "    1. **畳込み層**\n",
    "    2. **プーリング層**\n",
    "    3. **完全結合層**（通常のニューラルネットワークと正確に同じもの，CNN では最終 1 層または最終 1,2 層に用いる）\n",
    "\n",
    "入力信号はパラメータの値が異なる活性化関数によって非線形変換される。\n",
    "畳込み層とプーリング層と複数積み重ねることで多層化を実現し，深層ニューラルネットワークとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 畳み込み層\n",
    "畳込み層 convolution layers\n",
    "\n",
    "- 畳込み層のパラメータは学習可能なフィルタの組\n",
    "- 全フィルタは空間的に（幅と高さに沿って）小さくなる\n",
    "- フィルタは入力信号の深さと同一\n",
    "- 第1層のフィルタサイズは例えば $5\\times5\\times3$（$5$ 画素分の幅，高さ，と深さ $3$（３原色の色チャンネル）\n",
    "- 各層の順方向の計算は入力信号の幅と高さに沿って各フィルタを水平または垂直方向へスライド\n",
    "- フィルタの各値と入力信号の特定の位置の信号との内積（ドット積）。\n",
    "- 入力信号に沿って水平，垂直方向にフィルタをスライド\n",
    "- 各空間位置でフィルタの応答を定める 2 次元の活性化地図が生成される\n",
    "- 学習の結果獲得されるフィルタの形状には、方位検出器，色ブロッブ，生理学的には視覚野のニューロンの応答特性に類似\n",
    "- 上位層のフィルタには複雑な視覚パタンに対応する表象が獲得される\n",
    "- 各畳込み層全体では学習すべき入力信号をすべて網羅するフィルタの集合が形成される\n",
    "- 各フィルタは相異なる 2 次元の活性化地図を形成\n",
    "- 各フィルタの応答特性とみなすことが可能な活性化地図\n",
    "- フィルタの奥行き次元に沿って荷重総和を計算し、出力信号を生成\n",
    "\n",
    "<!--\n",
    "## パラメータ共有\n",
    "<font color=\"blue\">AlexNet</font>の論文では，第一畳込層は受容野サイズ $F=11$，\n",
    "ストライド $S=4$，ゼロパディングなし $P=0$。<br>\n",
    "畳込層 $K=96$ の深さ $(227-11)/4+1=55$。畳込層の出力サイズは [55x55x96]。55x55x96 ニューロンは入力領域 [11x11x3] と連結。全深度列 96 個のニューロンは同じ入力領域 [11×11×3] に繋がる。論文中では $(224-11)/4+1$ となっている。パディングについての記載はない。\n",
    "\n",
    "**パラメータ共有** パラメータ数を制御するために畳み込み層で使用される。上記の実世界の例を使用すると、最初の畳故意層には 55x55x96=290,400のニューロンがあり、それぞれ 11x11x3=363 の重みと1のバイアスがある。これにより CNN 単独の第 1 層に最大 290400x364=105,705,600 のパラメータが追加される。\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<center>\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_01.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_02.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_03.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_04.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_08.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_09.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_10.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_11.jpg\" style=\"width:49%\">\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_12.jpg\" style=\"width:49%\">\n",
    "</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* パディング/ストライド\n",
    "\n",
    "**空間配置**: 出力層ニューロンの数と配置については 3 つのハイパーパラメータで出力ニューロン数が定まる。\n",
    "\n",
    "1. 深さ(フィルタ数)\n",
    "2. ストライド幅\n",
    "3. ゼロパディング\n",
    "\n",
    "出力層ニューロン数のことを出力層の **深さ** 数と呼ぶハイパーパラメータである。深さ数とはフィルタ数（カーネル数）とも呼ばれる。第 1 畳込み層が生画像であれば，奥行き次元を構成する各ニューロンによって種々の方位を持つ線分(エッジ検出細胞)や色ブロッブのような特徴表現を獲得可能となる。入力の同じ領域を **深さ列** とするニューロン集団を **ファイバ** ともいう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* プーリング層 (Max pooling/average pooling)\n",
    "<center>\n",
    "<img src=\"2017Asakawa_KadokawaAscii_CNN1_Page_21.jpg\" style=\"width:49%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* チャネルと特徴マップ\n",
    "<center>\n",
    "<img src=\"weights.jpeg\" style=\"width:49%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 局所反応正規化 (Local Response Normalization; LRN)\n",
    "出力関数には $f\\left(x\\right)=\\left(1+\\exp\\left(-x\\right)\\right)^{-1}$ もしくは\n",
    "$f\\left(x\\right)=\\tanh\\left(x\\right)$ が伝統的に用いられてきた。\n",
    "しかし AlexNet では ReLU の出力 $a$ を以下のように局所的に規格化する変換が導入された。\n",
    "局所反応正規化 (Local Response Normalization: LRN) とは次式を用いて出力 $a$ は $b$ へと変換することである。\n",
    "\\begin{equation}\n",
    "b_{i}=\\frac{a_{i}}{\\left(\n",
    "\\kappa\n",
    "+\\alpha\\sum_{j=\\max\\left(0,i-n/2\\right)}^{\\min\\left(N-1,i+n/2\\right)}\\left(a_{j}\\right)^2\n",
    "\\right)^{\\beta}},\n",
    "\\end{equation}\n",
    "$n$ は隣接核関数地図における隣接する核関数数，$N$は層内の総核関数数である。\n",
    "$\\kappa=2$, $n=5$, $\\alpha=10^{-4}$, $\\beta=0.75$ はハイパーパラメータで定数であった。すなわち近傍の値を走査して自乗和し，その値を線形変換した値で規格化する。これは応答関数の値 $a$ が過剰に大きくなる場合を抑制する効果と推察される。S 次曲線を仮定したロジスティック関数やハイパータンジェントは上下限値に飽和するので局所反応正規化を用いる必要がない。\n",
    "\n",
    "AlexNet あるいは通常の CNN では，畳込み演算の領域である窓が重複する。$s$ 画素毎にプーリングユニットのグリッドが構成され，各プーリングユニットは $z\\times z$ の受容野を持つ。$s=z$ であれば重複しないプーリングとなる。$s<z$ であればオーバーラッププーリングとなる。AlexNet では $s=2$, $z=3$ であった。オーバーラッププーリングにより過学習が押さえらえるとされる。オーバラッププーリングは冗長性を担保し，認識システムを頑健にするという生物の生存にとっても意味があると考えられる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 全結合層\n",
    "\n",
    "* 畳み込み層の誤差逆伝播法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 基礎演習 (30分)\n",
    "\n",
    "* 畳み込み層 (L.Convolution2D)\n",
    "* パディング・ストライドの指定 (pad, stride)\n",
    "* プーリング層 (F.max\\_pooling, F.avg\\_pooling)\n",
    "* 画像の入力\n",
    "* 小課題：畳み込み・プーリングを使ってみる (使い方の学習)\n",
    "* 小課題：パラメータ数の計算と比較\n",
    "* Variableの形状変換 (F.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実践演習 (85分)\n",
    "#### 課題1: Chainerを用いたCNNの実装 (50分)\n",
    "* MNISTデータセットを用いて、CNNの学習に挑戦\n",
    "* 第6回までの知識と合わせて、正解率99%を目指そう\n",
    "* 使えるツール：畳み込み層、プーリング層、Batch Normalization、SGD、Adam、Dropout\n",
    "\n",
    "#### 課題2a：(パイロット用) Data Augmentation (20分)\n",
    "* Horizontal flippingの実装 (行列のスライス)\n",
    "* Scale augmentation (scipy.misc.imresize)\n",
    "* Random crop/padding (np.pad)\n",
    "\n",
    "#### 課題2b：(本番用) 未定 (20分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解説・コードレビュー (15分)\n",
    "\n",
    "### フィードバック・次回予告 (5分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献 (講師の方も随時追加お願いします)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
