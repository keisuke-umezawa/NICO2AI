{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICO2AI Lecture12 Reinforcement Learning II Practice\n",
    "\n",
    "## ChainerRL\n",
    "- website https://github.com/chainer/chainerrl\n",
    "\n",
    "ChainerRL is reinforcement learning library provided by PFN. Many kinds of algorithms are available and easily experimented on ALE or OpenAI Gym. Here is implemented algorithms.\n",
    "\n",
    "- DQN\n",
    "- Double DQN\n",
    "- Dueling DQN\n",
    "- DDPG\n",
    "- A3C\n",
    "- N-step Q Learning\n",
    "- PCL\n",
    "\n",
    "## CartPole with Deep Neural Network\n",
    "To learn how to use ChainerRL, let's implement DQN with fully connected layers powered by ChainerRL. Unlike CartPole with discretized state space previsouly you created, DQN is able to deal with continous state space because deep neural network approximate action values from direct inputs, what we call end-to-end. Fill blanks to complete DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "import gym\n",
    "from collections import deque\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# deep neural network with 3 fully connected layers\n",
    "class QFunction(chainer.Chain):\n",
    "    def __init__(self, num_state, num_actions, n_hidden_channels=50):\n",
    "        super().__init__(# code here)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # code here\n",
    "        # ues relu as activation function\n",
    "        # wrapper for discrete action space\n",
    "        return chainerrl.action_value.DiscreteActionValue(out)\n",
    "\n",
    "# ChainerRL also has replay memory used in DQN, but create original one here.\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = # code here\n",
    "    \n",
    "    def append(self, state, action, reward, next_state, next_action, is_state_terminal):\n",
    "        # code here\n",
    "        # single element is dictionary with\n",
    "        # state, action, reward, next_state, is_state_terminal\n",
    "        # as keys and values\n",
    "    \n",
    "    def sample(self, n):\n",
    "        return # code here\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def stop_current_episode(self):\n",
    "        pass\n",
    "    \n",
    "q_func = QFunction(4, 2)\n",
    "\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func)\n",
    "\n",
    "# ChainerRL provides some exploration strategies\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.1, random_action_func=lambda: np.random.choice(2))\n",
    "\n",
    "# Buffer size is 10^5 which is smaller than 10^6 used in the paper\n",
    "# because replay buffer actually occupies large memory space\n",
    "# replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 5)\n",
    "replay_buffer = ReplayBuffer(capacity=10 ** 5)\n",
    "\n",
    "discount = 0.9\n",
    "\n",
    "# DQN agent offered by ChainerRL with modular structure\n",
    "agent = chainerrl.agents.DQN(\n",
    "    q_func, optimizer, replay_buffer, discount, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=lambda x: np.array(x, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have DQN agent capable of predicting action values by end-to-end training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "# this may take a few minutes\n",
    "for i in range(1000):\n",
    "    state = env.reset()\n",
    "    sum_of_rewards = 0\n",
    "    reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = agent.act_and_train(state, reward)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        sum_of_rewards += reward\n",
    "        if done:\n",
    "            agent.stop_episode_and_train(state, reward, done)\n",
    "            break\n",
    "    rewards.append(sum_of_rewards)\n",
    "\n",
    "plt.plot(np.arange(1000), np.convolve(rewards, np.ones(10) / 10, mode='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check agent behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "\n",
    "## util function to render CartPole as GIF image\n",
    "def display_frames_as_gif(frames):\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "# evaluate trained agent\n",
    "state = env.reset()\n",
    "frames = []\n",
    "t = 0\n",
    "while True:\n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = agent.act(state)\n",
    "    state, _, done, _ = env.step(action)\n",
    "    t += 1\n",
    "    if done or t > 200:\n",
    "        break\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN on Atari\n",
    "Training DQN for Atari games seriously takes a long time. So in this lecture, you just check behaviour of a trained model on Pong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from chainerrl import links\n",
    "\n",
    "\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "\n",
    "# ChainerRL provides some predefined networks\n",
    "# in this case, q_func has 3 convolutional layers and 2 fully connected layers\n",
    "# n_input_channels represents the number of images to feed networks\n",
    "convs = links.NatureDQNHead(n_input_channels=4)\n",
    "q_func = links.Sequence(convs, L.Linear(512, 6),\n",
    "        chainerrl.action_value.DiscreteActionValue)\n",
    "\n",
    "# create optimizer to avoid errors when loading trained models\n",
    "# but this RMSprop optimizer is actually same as the one in the paper\n",
    "optimizer = chainer.optimizers.RMSpropGraves(\n",
    "        lr=2.5e-4, alpha=0.95, momentum=0.0, eps=1e-2)\n",
    "optimizer.setup(q_func)\n",
    "\n",
    "# most of arguments are None because agent would not train here\n",
    "agent = chainerrl.agents.DQN(\n",
    "    q_func, optimizer, None, None, None,\n",
    "    phi=lambda x: np.array(x, dtype=np.float32) / 255.0)\n",
    "\n",
    "# load a trained model\n",
    "agent.load(os.path.join(os.getcwd(), 'trained'))\n",
    "\n",
    "# deque object to hold 4 frames for frame skipping\n",
    "states = deque(np.zeros((4, 84, 84)).tolist(), maxlen=4)\n",
    "\n",
    "# evaluate trained agent\n",
    "state = env.reset()\n",
    "frames = []\n",
    "t = 0\n",
    "while True:\n",
    "    # process input images\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "    state = cv2.resize(state, (84, 84))\n",
    "    states.append(state)\n",
    "    \n",
    "    frames.append(env.render(mode='rgb_array'))\n",
    "    action = agent.act(list(reversed(states)))\n",
    "    state, _, done, _ = env.step(action)\n",
    "    t += 1\n",
    "    if done or t > 200:\n",
    "        break\n",
    "env.render(close=True)\n",
    "display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_conv_layer(weight, shape=(8, 8)):\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    for i in range(weight.shape[0]):\n",
    "        ax = fig.add_subplot(shape[0], shape[1], i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(weight[i][0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# visualize the first convolutional layer\n",
    "visualize_conv_layer(convs[0].W.data, (4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the second convolutional layer\n",
    "visualize_conv_layer(convs[1].W.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize the third convolutional layer\n",
    "visualize_conv_layer(convs[2].W.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_activations(activations, shape=(8, 8)):\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "    for i in range(activations.shape[0]):\n",
    "        ax = fig.add_subplot(shape[0], shape[1], i + 1, xticks=[], yticks=[])\n",
    "        ax.imshow(activations[i], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "states = deque(np.zeros((4, 84, 84)).tolist(), maxlen=4)\n",
    "env.reset()\n",
    "for i in range(15):\n",
    "    state, _, _, _ = env.step(0)\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)\n",
    "    state = cv2.resize(state, (84, 84))\n",
    "    states.append(state)\n",
    "\n",
    "# visualize activations on the first layer\n",
    "h = F.relu(convs[0](np.array([list(reversed(states))], dtype=np.float32) / 255.0))\n",
    "visualize_activations(h.data[0], (4, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize activations on the second layer\n",
    "h = F.relu(convs[1](h))\n",
    "visualize_activations(h.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize activations on the third layer\n",
    "h = F.relu(convs[2](h))\n",
    "visualize_activations(h.data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
