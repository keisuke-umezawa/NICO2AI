# 第2回：機械学習入門

## 到達目標
* 機械学習の目的及び種類とその評価方法を理解する。
* 第1回に引き続き、numpyで頻出する関数の使い方を実践的に理解する。
* k-NN法を理解し、それをnumpyを用いて実装できる。

## キーワード
* 教師あり学習
* 教師なし学習
* 強化学習
* 訓練データ、テストデータ、バリデーションデータ
* 交差検証 (CV)
* 適合率、再現率、F値、正解率
* 過学習
* 汎化誤差
* np.sum, np.mean, np.var
* np.random
* Advanced Indexing
* np.sort, np.argsort, np.argpartition
* k-Nearest Neighbor (k-NN) 法
* コサイン類似度

## タイムスケジュール
### 前回の復習 (5分)
### 講義(65分)
#### Part 1: 機械学習の基礎 (35分)
* 機械学習アルゴリズムの分類 (教師あり/教師なし/強化学習)
* それぞれの代表的なアルゴリズムと用途の紹介
* 機械学習アルゴリズムをどのように評価するか？
* 訓練データでテストして精度100%的な話でも
* データの分割
* 交差検証の目的とやり方、長所短所
* LOO法の解説 (解説のみ)
* 評価の各指標 (適合率、再現率、F値、正解率)
* 過学習の解説とその対処
* 汎化誤差の定義と、CVとの関係
* アルゴリズムの解説：k-NN法
#### Part 2: numpy入門 (2) (30分)
* 行列積・内積 (np.matmul, np.dot)
* 総和、平均、最大値、最小値(np.sum, np.mean, np.max, np.min)
* L2ノルム (np.linalg.norm)
* 数学関数 (np.sin, np.cos, np.tah, np.exp, np.log など)
* 乱数生成 (np.random.random, np.random.normal)
* ユークリッド類似度・コサイン類似度

### 基礎演習(20分)
* 小課題1: 教師あり学習、教師なし学習、強化学習の違いを説明し、自分の身の回りのデータではどの手法を使えば良いか考えよ。
* 小課題2: コサイン類似度の計算を実装せよ

### 実践演習(90分)
#### 課題1: 単層パーセプトロン (20分)
**注：lecture1より移動**
* パーセプトロンの学習アルゴリズム
* 順伝播の計算の実装

#### 課題2：k-NN法を用いたMNISTの学習 (50分)
##### 事前に実装済み
* MNISTの読み込み
* データの可視化 (MNISTの数字の可視化)
* 訓練/テストコードの骨子
* 適合率・再現率・F値の出力

##### 講義中に実装
* 訓練データとバリデーションデータの分割
* 最近傍の検索

##### 補足
* 余裕がある人はsklearn.model\_selection.KFoldで交差検証
* 余裕のある人向け：kの値を変えて、最適なkを探す

#### 解説・コードレビュー (15分)
* できた人のコードを読んで、良い点・改善点があれば指摘
* そのうえで答えの解説

#### フィードバック・次回予告 (5分)

## 参考文献 (随時追加してください)
### 統計的機械学習入門
https://www.nii.ac.jp/userdata/karuizawa/h23/111104\_3rdlecueda.pdf

### 朱鷺の社wiki - 交差確認 (cross validation)
http://ibisforest.org/index.php?%E4%BA%A4%E5%B7%AE%E7%A2%BA%E8%AA%8D

### 統計的機械学習入門 評価方法
http://www.r.dl.itc.u-tokyo.ac.jp/~nakagawa/SML1/eval1.pdf
